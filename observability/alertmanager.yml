global:
  resolve_timeout: 5m
  slack_api_url: ${SLACK_WEBHOOK_URL}

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'default'
  
  routes:
    - match:
        severity: critical
      receiver: critical
      continue: true
    
    - match:
        service: api
      receiver: api-team
      continue: true
    
    - match:
        service: ml-worker
      receiver: ml-team
      continue: true

receivers:
  - name: 'default'
    webhook_configs:
      - url: 'http://localhost:5001/webhook'
        send_resolved: true

  - name: 'critical'
    slack_configs:
      - channel: '#alerts-critical'
        title: 'ðŸš¨ Critical Alert'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}{{ end }}'
        send_resolved: true
    
    pagerduty_configs:
      - service_key: ${PAGERDUTY_SERVICE_KEY}
        severity: critical

  - name: 'api-team'
    email_configs:
      - to: 'api-team@ooblex.com'
        from: 'alerts@ooblex.com'
        smarthost: 'smtp.gmail.com:587'
        auth_username: 'alerts@ooblex.com'
        auth_password: ${EMAIL_PASSWORD}
        headers:
          Subject: 'Ooblex API Alert: {{ .GroupLabels.alertname }}'

  - name: 'ml-team'
    slack_configs:
      - channel: '#ml-alerts'
        title: 'ðŸ¤– ML Worker Alert'
        text: '{{ .CommonAnnotations.summary }}'
        send_resolved: true

inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'cluster', 'service']

templates:
  - '/etc/alertmanager/templates/*.tmpl'